"""
Galaxy Script æ‰¹é‡è¯­æ³•éªŒè¯å·¥å…·ï¼ˆå«è¯­ä¹‰åˆ†æï¼‰
ç”¨æ³•: python validate_galaxy.py
"""

import os
import re
import sys
from datetime import datetime
from typing import Optional
from lark import Lark, exceptions

# â”€â”€ è·¯å¾„é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GRAMMAR_FILE = r"D:\galaxyscript\ANSI C95_V2.lark"
SCRIPTS_DIR  = r"D:\galaxyscript\galaxy_scripts"
LOG_FILE     = r"D:\galaxyscript\validation_errors.log"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# å·²çŸ¥å†…ç½®ç±»å‹ï¼Œé¢„å¤„ç†æ—¶ä¸æ›¿æ¢è¿™äº›
BUILTIN_TYPES = {
    'void', 'int', 'fixed', 'bool', 'string',
    'unitfilter', 'unitgroup', 'unit', 'point', 'timer',
    'region', 'trigger', 'wave', 'actor', 'revealer',
    'playergroup', 'text', 'sound', 'soundlink', 'color',
    'abilcmd', 'order', 'marker', 'bank', 'camerainfo',
    'actorscope', 'aifilter', 'wavetarget', 'effecthistory',
    'bitmask', 'datetime', 'doodad', 'generichandle',
    'transmissionsource', 'unitref', 'waveinfo', 'entryset',
    'boolean', 'integer',
}


# â”€â”€ è¯­æ³•è§£æç›¸å…³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_grammar(grammar_path: str) -> Lark:
    """åŠ è½½ lark è¯­æ³•æ–‡ä»¶ï¼Œè¿”å›è§£æå™¨ï¼ˆå¼€å¯è¡Œåˆ—å·è®°å½•ï¼‰"""
    with open(grammar_path, "r", encoding="utf-8") as f:
        grammar = f.read()
    # return Lark(grammar, parser="lalr", propagate_positions=True)
    return Lark(grammar, parser="earley", ambiguity="resolve", propagate_positions=True)


def collect_scripts(scripts_dir: str) -> list:
    """é€’å½’æ”¶é›†ç›®å½•ä¸‹æ‰€æœ‰ .galaxy æ–‡ä»¶"""
    results = []
    for root, _, files in os.walk(scripts_dir):
        for name in files:
            if name.endswith(".galaxy"):
                results.append(os.path.join(root, name))
    return sorted(results)


def collect_all_type_names(scripts: list) -> set:
    """ç¬¬ä¸€éæ‰«ææ‰€æœ‰æ–‡ä»¶ï¼Œç”¨æ­£åˆ™æ”¶é›†ç”¨æˆ·è‡ªå®šä¹‰ç±»å‹åã€‚"""
    type_names = set()
    for filepath in scripts:
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                source = f.read()
        except Exception:
            continue
        for m in re.finditer(r'\bstruct\s+([a-zA-Z_][a-zA-Z0-9_]*)', source):
            type_names.add(m.group(1))
        for m in re.finditer(r'\btypedef\s+\S+\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*;', source):
            type_names.add(m.group(1))
    type_names -= BUILTIN_TYPES
    return type_names


def preprocess(source: str, type_names: set) -> str:
    """é¢„å¤„ç†ï¼šæ›¿æ¢è‡ªå®šä¹‰ç±»å‹åå’Œ structref<T>"""
    if type_names:
        pattern = r'\b(' + '|'.join(
            re.escape(n) for n in sorted(type_names, key=len, reverse=True)
        ) + r')\b'
        source = re.sub(pattern, 'int', source)
    source = re.sub(r'\bstructref\s*<[^>]+>', 'int', source)
    return source


def classify_syntax_error(e: exceptions.UnexpectedToken) -> str:
    token_str = str(e.token)
    if token_str == '':
        return f"[æ–‡ä»¶æˆªæ–­] æ–‡ä»¶åœ¨ line {e.line}, col {e.column} å¤„æ„å¤–ç»“æŸ"
    return (
        f"UnexpectedToken '{e.token}' at line {e.line}, col {e.column}\n"
        f"  Expected: {e.expected}"
    )


# â”€â”€ è¯­ä¹‰åˆ†æç›¸å…³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_semantic_analysis(tree, global_table) -> list:
    """
    åœ¨è¯­æ³•æ ‘ä¸Šè¿è¡Œä½œç”¨åŸŸåˆ†æï¼Œè¿”å›è¯­ä¹‰é”™è¯¯åˆ—è¡¨ã€‚
    å¯¼å…¥å»¶è¿Ÿåˆ°è¿™é‡Œï¼Œé¿å…å¾ªç¯ä¾èµ–ã€‚
    """
    from scope_analyzer import ScopeAnalyzer
    analyzer = ScopeAnalyzer(global_table)
    analyzer.visit(tree)
    return analyzer.errors


def build_global_symbol_table(trees: list):
    """
    å¯¹æ‰€æœ‰æ–‡ä»¶çš„ AST åšç¬¬ä¸€éæ‰«æï¼Œå»ºç«‹å…¨å±€ç¬¦å·è¡¨ã€‚
    trees: [(filepath, tree), ...]
    è¿”å› (SymbolTable, [æ”¶é›†æ—¶å‘ç°çš„é”™è¯¯])
    """
    from symbol_collector import SymbolCollector, SymbolTable

    # åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„ç¬¦å·åˆ°ä¸€å¼ å…¨å±€è¡¨
    merged_table = SymbolTable()
    all_collector_errors = []

    for filepath, tree in trees:
        collector = SymbolCollector()
        collector.visit(tree)

        # æŠŠæ”¶é›†åˆ°çš„ç¬¦å·åˆå¹¶è¿›å…¨å±€è¡¨
        for name, info in collector.table.symbols.items():
            existing = merged_table.declare(info)
            # è·¨æ–‡ä»¶çš„é‡å¤åªå¯¹å‡½æ•°å®šä¹‰æŠ¥é”™ï¼Œå˜é‡é‡å¤å¿½ç•¥ï¼ˆå¯èƒ½æ˜¯å¤´æ–‡ä»¶å¤šæ¬¡åŒ…å«ï¼‰

        for name, fields in collector.table.structs.items():
            merged_table.declare_struct(name, fields)

        # æ”¶é›†ç¬¦å·æ”¶é›†é˜¶æ®µå‘ç°çš„é”™è¯¯ï¼ˆåŒæ–‡ä»¶å†…é‡å¤å£°æ˜ï¼‰
        for err in collector.errors:
            all_collector_errors.append((filepath, err))

    return merged_table, all_collector_errors


# â”€â”€ å•æ–‡ä»¶éªŒè¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class FileResult:
    """å•ä¸ªæ–‡ä»¶çš„éªŒè¯ç»“æœ"""
    def __init__(self, filepath: str):
        self.filepath        = filepath
        self.syntax_error    = None    # str æˆ– None
        self.semantic_errors = []      # [ScopeError, ...]
        self.is_truncated    = False

    @property
    def has_error(self) -> bool:
        return self.syntax_error is not None or bool(self.semantic_errors)

    @property
    def status(self) -> str:
        if self.is_truncated:
            return "TRUNCATE"
        if self.syntax_error:
            return "SYNTAX  "
        if self.semantic_errors:
            return "SEMANTIC"
        return "OK      "


def parse_file(parser: Lark, filepath: str, type_names: set):
    """
    è¯­æ³•è§£æå•ä¸ªæ–‡ä»¶ï¼Œè¿”å› (tree, syntax_error_str, is_truncated)ã€‚
    tree ä¸º None è¡¨ç¤ºè§£æå¤±è´¥ã€‚
    """
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            source = f.read()
        source = preprocess(source, type_names)
        tree = parser.parse(source)
        return tree, None, False

    except exceptions.UnexpectedCharacters as e:
        err = (
            f"UnexpectedCharacters at line {e.line}, col {e.column}\n"
            f"  Expected: {e.expected}\n"
            f"  Context : {repr(e.char)}"
        )
        return None, err, False

    except exceptions.UnexpectedToken as e:
        err = classify_syntax_error(e)
        truncated = str(e.token) == ''
        return None, err, truncated

    except exceptions.UnexpectedEOF as e:
        return None, f"[æ–‡ä»¶æˆªæ–­] æ–‡ä»¶æ„å¤–ç»“æŸ\n  Expected: {e.expected}", True

    except Exception as e:
        return None, f"{type(e).__name__}: {e}", False


# â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    # 1. åŠ è½½è¯­æ³•
    print(f"æ­£åœ¨åŠ è½½è¯­æ³•æ–‡ä»¶: {GRAMMAR_FILE}")
    try:
        parser = load_grammar(GRAMMAR_FILE)
    except Exception as e:
        print(f"[ERROR] è¯­æ³•æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    print("è¯­æ³•æ–‡ä»¶åŠ è½½æˆåŠŸ\n")

    # 2. æ”¶é›†è„šæœ¬
    scripts = collect_scripts(SCRIPTS_DIR)
    if not scripts:
        print(f"[WARN] æœªæ‰¾åˆ°ä»»ä½• .galaxy æ–‡ä»¶: {SCRIPTS_DIR}")
        sys.exit(0)
    print(f"å…±æ‰¾åˆ° {len(scripts)} ä¸ªæ–‡ä»¶\n")

    # 3. å…¨å±€æ”¶é›†è‡ªå®šä¹‰ç±»å‹åï¼ˆé¢„å¤„ç†ç”¨ï¼‰
    print("æ­£åœ¨æ”¶é›†è‡ªå®šä¹‰ç±»å‹å...")
    type_names = collect_all_type_names(scripts)
    print(f"æ”¶é›†åˆ° {len(type_names)} ä¸ªè‡ªå®šä¹‰ç±»å‹å\n")

    # 4. ç¬¬ä¸€éï¼šè¯­æ³•è§£æï¼Œæ”¶é›†æ‰€æœ‰ AST
    print("ç¬¬ä¸€éï¼šè¯­æ³•è§£æ...")
    results = {}
    valid_trees = []   # [(filepath, tree), ...]

    for i, filepath in enumerate(scripts, 1):
        rel = os.path.relpath(filepath, SCRIPTS_DIR)
        result = FileResult(filepath)
        tree, syntax_err, truncated = parse_file(parser, filepath, type_names)

        if syntax_err:
            result.syntax_error = syntax_err
            result.is_truncated = truncated
            print(f"  [{i:>4}/{len(scripts)}] {result.status} {rel}")
        else:
            valid_trees.append((filepath, tree))
            print(f"  [{i:>4}/{len(scripts)}] OK       {rel}")

        results[filepath] = result

    syntax_fail = sum(1 for r in results.values() if r.syntax_error)
    print(f"\nè¯­æ³•è§£æå®Œæˆ: {len(scripts) - syntax_fail} é€šè¿‡ / {syntax_fail} å¤±è´¥\n")

    # 5. ç¬¬äºŒéï¼šå»ºç«‹å…¨å±€ç¬¦å·è¡¨
    print("ç¬¬äºŒéï¼šå»ºç«‹å…¨å±€ç¬¦å·è¡¨...")
    global_table, collector_errors = build_global_symbol_table(valid_trees)
    print(f"æ”¶é›†åˆ° {len(global_table.symbols)} ä¸ªå…¨å±€ç¬¦å·\n")

    # æŠŠç¬¦å·æ”¶é›†é˜¶æ®µçš„é”™è¯¯é™„åŠ åˆ°å¯¹åº”æ–‡ä»¶
    for filepath, err_dict in collector_errors:
        if filepath in results:
            from scope_analyzer import ScopeError
            results[filepath].semantic_errors.append(ScopeError(
                kind=err_dict['kind'],
                message=err_dict['message'],
                line=err_dict['line'],
                col=err_dict['col'],
            ))

    # 6. ç¬¬ä¸‰éï¼šä½œç”¨åŸŸåˆ†æ
    print("ç¬¬ä¸‰éï¼šä½œç”¨åŸŸè¯­ä¹‰åˆ†æ...")
    for i, (filepath, tree) in enumerate(valid_trees, 1):
        rel = os.path.relpath(filepath, SCRIPTS_DIR)
        try:
            semantic_errors = run_semantic_analysis(tree, global_table)
            results[filepath].semantic_errors.extend(semantic_errors)
            status = "SEMANTIC" if semantic_errors else "OK      "
            print(f"  [{i:>4}/{len(valid_trees)}] {status} {rel}"
                  + (f" ({len(semantic_errors)} ä¸ªé—®é¢˜)" if semantic_errors else ""))
        except Exception as e:
            print(f"  [{i:>4}/{len(valid_trees)}] ERROR    {rel} (è¯­ä¹‰åˆ†æå¼‚å¸¸: {e})")

    # 7. æ±‡æ€»
    syntax_errors   = [r for r in results.values() if r.syntax_error and not r.is_truncated]
    truncated_files = [r for r in results.values() if r.is_truncated]
    semantic_errors = [r for r in results.values() if r.semantic_errors]
    ok_files        = [r for r in results.values() if not r.has_error]

    print(f"\n{'='*60}")
    print(f"éªŒè¯å®Œæˆ: {len(scripts)} ä¸ªæ–‡ä»¶")
    print(f"  âœ… é€šè¿‡:     {len(ok_files)} ä¸ª")
    print(f"  âŒ è¯­æ³•é”™è¯¯: {len(syntax_errors)} ä¸ª")
    print(f"  âš ï¸  æ–‡ä»¶æˆªæ–­: {len(truncated_files)} ä¸ª")
    print(f"  ğŸ” è¯­ä¹‰é—®é¢˜: {len(semantic_errors)} ä¸ªæ–‡ä»¶ï¼Œ"
          f"å…± {sum(len(r.semantic_errors) for r in semantic_errors)} å¤„")
    print(f"{'='*60}\n")

    if not any(r.has_error for r in results.values()):
        print("å…¨éƒ¨é€šè¿‡ï¼Œæ— é”™è¯¯ã€‚")
        return

    # 8. å†™å…¥ log
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "w", encoding="utf-8") as log:
        log.write("Galaxy Script è¯­æ³•+è¯­ä¹‰éªŒè¯æŠ¥å‘Š\n")
        log.write(f"ç”Ÿæˆæ—¶é—´ : {timestamp}\n")
        log.write(f"è¯­æ³•æ–‡ä»¶ : {GRAMMAR_FILE}\n")
        log.write(f"è„šæœ¬ç›®å½• : {SCRIPTS_DIR}\n")
        log.write(
            f"æ€»è®¡     : {len(scripts)} ä¸ªæ–‡ä»¶ / "
            f"è¯­æ³•é”™è¯¯ {len(syntax_errors)} ä¸ª / "
            f"æ–‡ä»¶æˆªæ–­ {len(truncated_files)} ä¸ª / "
            f"è¯­ä¹‰é—®é¢˜ {len(semantic_errors)} ä¸ªæ–‡ä»¶\n"
        )
        log.write("=" * 72 + "\n\n")

        # è¯­æ³•é”™è¯¯
        if syntax_errors:
            log.write("ã€è¯­æ³•é”™è¯¯ã€‘\n")
            log.write("=" * 72 + "\n\n")
            for r in syntax_errors:
                rel = os.path.relpath(r.filepath, SCRIPTS_DIR)
                log.write(f"FILE: {rel}\n")
                log.write(f"PATH: {r.filepath}\n")
                log.write(f"{r.syntax_error}\n")
                log.write("-" * 72 + "\n\n")

        # æ–‡ä»¶æˆªæ–­
        if truncated_files:
            log.write("ã€æ–‡ä»¶æˆªæ–­ï¼ˆå†…å®¹ä¸å®Œæ•´ï¼Œéè¯­æ³•é”™è¯¯ï¼‰ã€‘\n")
            log.write("=" * 72 + "\n\n")
            for r in truncated_files:
                rel = os.path.relpath(r.filepath, SCRIPTS_DIR)
                log.write(f"FILE: {rel}\n")
                log.write(f"PATH: {r.filepath}\n")
                log.write(f"{r.syntax_error}\n")
                log.write("-" * 72 + "\n\n")

        # è¯­ä¹‰é—®é¢˜
        if semantic_errors:
            log.write("ã€è¯­ä¹‰é—®é¢˜ã€‘\n")
            log.write("=" * 72 + "\n\n")
            for r in semantic_errors:
                rel = os.path.relpath(r.filepath, SCRIPTS_DIR)
                log.write(f"FILE: {rel}\n")
                log.write(f"PATH: {r.filepath}\n")
                for err in r.semantic_errors:
                    log.write(f"  {err}\n")
                log.write("-" * 72 + "\n\n")

    print(f"é”™è¯¯è¯¦æƒ…å·²å†™å…¥: {LOG_FILE}")


if __name__ == "__main__":
    main()
